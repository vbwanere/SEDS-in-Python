{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Gaussian Mixture Model Sine Curve\n",
        "\n",
        "This example demonstrates the behavior of Gaussian mixture models fit on data\n",
        "that was not sampled from a mixture of Gaussian random variables. The dataset\n",
        "is formed by 100 points loosely spaced following a noisy sine curve. There is\n",
        "therefore no ground truth value for the number of Gaussian components.\n",
        "\n",
        "The first model is a classical Gaussian Mixture Model with 10 components fit\n",
        "with the Expectation-Maximization algorithm.\n",
        "\n",
        "The second model is a Bayesian Gaussian Mixture Model with a Dirichlet process\n",
        "prior fit with variational inference. The low value of the concentration prior\n",
        "makes the model favor a lower number of active components. This models\n",
        "\"decides\" to focus its modeling power on the big picture of the structure of\n",
        "the dataset: groups of points with alternating directions modeled by\n",
        "non-diagonal covariance matrices. Those alternating directions roughly capture\n",
        "the alternating nature of the original sine signal.\n",
        "\n",
        "The third model is also a Bayesian Gaussian mixture model with a Dirichlet\n",
        "process prior but this time the value of the concentration prior is higher\n",
        "giving the model more liberty to model the fine-grained structure of the data.\n",
        "The result is a mixture with a larger number of active components that is\n",
        "similar to the first model where we arbitrarily decided to fix the number of\n",
        "components to 10.\n",
        "\n",
        "Which model is the best is a matter of subjective judgment: do we want to\n",
        "favor models that only capture the big picture to summarize and explain most of\n",
        "the structure of the data while ignoring the details or do we prefer models\n",
        "that closely follow the high density regions of the signal?\n",
        "\n",
        "The last two panels show how we can sample from the last two models. The\n",
        "resulting samples distributions do not look exactly like the original data\n",
        "distribution. The difference primarily stems from the approximation error we\n",
        "made by using a model that assumes that the data was generated by a finite\n",
        "number of Gaussian components instead of a continuous noisy sine curve.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'GaussianMixture' object has no attribute 'm'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 84\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39m\"\"\"gmm = mixture.GaussianMixture(\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39m    n_components=10, covariance_type=\"full\", max_iter=100\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[39m).fit(X)\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     82\u001b[0m gmm \u001b[39m=\u001b[39m GaussianMixture(k\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, max_iter\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[1;32m     83\u001b[0m plot_results(\n\u001b[0;32m---> 84\u001b[0m     X, gmm\u001b[39m.\u001b[39;49mpredict(X), gmm\u001b[39m.\u001b[39mmeans, gmm\u001b[39m.\u001b[39mcovariances, \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mExpectation-maximization\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m )\n\u001b[1;32m     87\u001b[0m dpgmm \u001b[39m=\u001b[39m mixture\u001b[39m.\u001b[39mBayesianGaussianMixture(\n\u001b[1;32m     88\u001b[0m     n_components\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[1;32m     89\u001b[0m     covariance_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfull\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m     random_state\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m     97\u001b[0m )\u001b[39m.\u001b[39mfit(X)\n\u001b[1;32m     98\u001b[0m plot_results(\n\u001b[1;32m     99\u001b[0m     X,\n\u001b[1;32m    100\u001b[0m     dpgmm\u001b[39m.\u001b[39mpredict(X),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfor $\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mgamma_0=0.01$.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    106\u001b[0m )\n",
            "File \u001b[0;32m~/OneDrive/GitHub_repos/GitHub_repos_python/seds_in_jax/gmr_lib/gaussian_mixture.py:65\u001b[0m, in \u001b[0;36mGaussianMixture.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m---> 65\u001b[0m     responsibility_k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresponsibility_matrix(X)\n\u001b[1;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39margmax(responsibility_k, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
            "File \u001b[0;32m~/OneDrive/GitHub_repos/GitHub_repos_python/seds_in_jax/gmr_lib/gaussian_mixture.py:54\u001b[0m, in \u001b[0;36mGaussianMixture.responsibility_matrix\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mresponsibility_matrix\u001b[39m(\u001b[39mself\u001b[39m, X): \u001b[39m# shape = (m, k)\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     likelihood \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mm, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk))\n\u001b[1;32m     55\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk):\n\u001b[1;32m     56\u001b[0m         likelihood[:, i] \u001b[39m=\u001b[39m multivariate_normal\u001b[39m.\u001b[39mpdf(X, mean\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmu[i],\\\n\u001b[1;32m     57\u001b[0m                                            cov\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigma[i])\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'GaussianMixture' object has no attribute 'm'"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "from scipy import linalg\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "from sklearn import mixture\n",
        "from gaussian_mixture import GaussianMixture\n",
        "\n",
        "color_iter = itertools.cycle([\"navy\", \"c\", \"cornflowerblue\", \"gold\", \"darkorange\"])\n",
        "\n",
        "\n",
        "def plot_results(X, Y, means, covariances, index, title):\n",
        "    splot = plt.subplot(5, 1, 1 + index)\n",
        "    for i, (mean, covar, color) in enumerate(zip(means, covariances, color_iter)):\n",
        "        v, w = linalg.eigh(covar)\n",
        "        v = 2.0 * np.sqrt(2.0) * np.sqrt(v)\n",
        "        u = w[0] / linalg.norm(w[0])\n",
        "        # as the DP will not use every component it has access to\n",
        "        # unless it needs it, we shouldn't plot the redundant\n",
        "        # components.\n",
        "        if not np.any(Y == i):\n",
        "            continue\n",
        "        plt.scatter(X[Y == i, 0], X[Y == i, 1], 0.8, color=color)\n",
        "\n",
        "        # Plot an ellipse to show the Gaussian component\n",
        "        angle = np.arctan(u[1] / u[0])\n",
        "        angle = 180.0 * angle / np.pi  # convert to degrees\n",
        "        ell = mpl.patches.Ellipse(mean, v[0], v[1], angle=180.0 + angle, color=color)\n",
        "        ell.set_clip_box(splot.bbox)\n",
        "        ell.set_alpha(0.5)\n",
        "        splot.add_artist(ell)\n",
        "\n",
        "    plt.xlim(-6.0, 4.0 * np.pi - 6.0)\n",
        "    plt.ylim(-5.0, 5.0)\n",
        "    plt.title(title)\n",
        "    plt.xticks(())\n",
        "    plt.yticks(())\n",
        "\n",
        "\n",
        "def plot_samples(X, Y, n_components, index, title):\n",
        "    plt.subplot(5, 1, 4 + index)\n",
        "    for i, color in zip(range(n_components), color_iter):\n",
        "        # as the DP will not use every component it has access to\n",
        "        # unless it needs it, we shouldn't plot the redundant\n",
        "        # components.\n",
        "        if not np.any(Y == i):\n",
        "            continue\n",
        "        plt.scatter(X[Y == i, 0], X[Y == i, 1], 0.8, color=color)\n",
        "\n",
        "    plt.xlim(-6.0, 4.0 * np.pi - 6.0)\n",
        "    plt.ylim(-5.0, 5.0)\n",
        "    plt.title(title)\n",
        "    plt.xticks(())\n",
        "    plt.yticks(())\n",
        "\n",
        "\n",
        "# Parameters\n",
        "n_samples = 100\n",
        "\n",
        "# Generate random sample following a sine curve\n",
        "np.random.seed(0)\n",
        "X = np.zeros((n_samples, 2))\n",
        "step = 4.0 * np.pi / n_samples\n",
        "\n",
        "for i in range(X.shape[0]):\n",
        "    x = i * step - 6.0\n",
        "    X[i, 0] = x + np.random.normal(0, 0.1)\n",
        "    X[i, 1] = 3.0 * (np.sin(x) + np.random.normal(0, 0.2))\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.subplots_adjust(\n",
        "    bottom=0.04, top=0.95, hspace=0.2, wspace=0.05, left=0.03, right=0.97\n",
        ")\n",
        "\n",
        "# Fit a Gaussian mixture with EM using ten components\n",
        "\"\"\"gmm = mixture.GaussianMixture(\n",
        "    n_components=10, covariance_type=\"full\", max_iter=100\n",
        ").fit(X)\n",
        "\"\"\"\n",
        "gmm = GaussianMixture(k=10, max_iter=100)\n",
        "plot_results(\n",
        "    X, gmm.predict(X), gmm.means, gmm.covariances, 0, \"Expectation-maximization\"\n",
        ")\n",
        "\n",
        "dpgmm = mixture.BayesianGaussianMixture(\n",
        "    n_components=10,\n",
        "    covariance_type=\"full\",\n",
        "    weight_concentration_prior=1e-2,\n",
        "    weight_concentration_prior_type=\"dirichlet_process\",\n",
        "    mean_precision_prior=1e-2,\n",
        "    covariance_prior=1e0 * np.eye(2),\n",
        "    init_params=\"random\",\n",
        "    max_iter=100,\n",
        "    random_state=2,\n",
        ").fit(X)\n",
        "plot_results(\n",
        "    X,\n",
        "    dpgmm.predict(X),\n",
        "    dpgmm.means_,\n",
        "    dpgmm.covariances_,\n",
        "    1,\n",
        "    \"Bayesian Gaussian mixture models with a Dirichlet process prior \"\n",
        "    r\"for $\\gamma_0=0.01$.\",\n",
        ")\n",
        "\n",
        "X_s, y_s = dpgmm.sample(n_samples=2000)\n",
        "plot_samples(\n",
        "    X_s,\n",
        "    y_s,\n",
        "    dpgmm.n_components,\n",
        "    0,\n",
        "    \"Gaussian mixture with a Dirichlet process prior \"\n",
        "    r\"for $\\gamma_0=0.01$ sampled with $2000$ samples.\",\n",
        ")\n",
        "\n",
        "dpgmm = mixture.BayesianGaussianMixture(\n",
        "    n_components=10,\n",
        "    covariance_type=\"full\",\n",
        "    weight_concentration_prior=1e2,\n",
        "    weight_concentration_prior_type=\"dirichlet_process\",\n",
        "    mean_precision_prior=1e-2,\n",
        "    covariance_prior=1e0 * np.eye(2),\n",
        "    init_params=\"kmeans\",\n",
        "    max_iter=100,\n",
        "    random_state=2,\n",
        ").fit(X)\n",
        "plot_results(\n",
        "    X,\n",
        "    dpgmm.predict(X),\n",
        "    dpgmm.means_,\n",
        "    dpgmm.covariances_,\n",
        "    2,\n",
        "    \"Bayesian Gaussian mixture models with a Dirichlet process prior \"\n",
        "    r\"for $\\gamma_0=100$\",\n",
        ")\n",
        "\n",
        "X_s, y_s = dpgmm.sample(n_samples=2000)\n",
        "plot_samples(\n",
        "    X_s,\n",
        "    y_s,\n",
        "    dpgmm.n_components,\n",
        "    1,\n",
        "    \"Gaussian mixture with a Dirichlet process prior \"\n",
        "    r\"for $\\gamma_0=100$ sampled with $2000$ samples.\",\n",
        ")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
